\documentclass[letterpaper,12pt]{article}
\usepackage{array}
\usepackage{threeparttable}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1.25in,rmargin=1.25in}
\usepackage{fancyhdr,lastpage}
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
\lfoot{}
\cfoot{}
\rfoot{\footnotesize\textsl{Page \thepage\ of \pageref{LastPage}}}
\renewcommand\headrulewidth{0pt}
\renewcommand\footrulewidth{0pt}
\usepackage[format=hang,font=normalsize,labelfont=bf]{caption}
\usepackage{listings}
\lstset{frame=single,
  language=Python,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{harvard}
\usepackage{setspace}
\usepackage{float,color}
\usepackage[pdftex]{graphicx}
\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor=red,urlcolor=blue}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{derivation}{Derivation} % Number derivations on their own
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}{Proposition} % Number propositions on their own
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
%\numberwithin{equation}{section}
\bibliographystyle{aer}
\newcommand\ve{\varepsilon}
\newcommand\boldline{\arrayrulewidth{1pt}\hline}


\begin{document}

\begin{flushleft}
  \textbf{\large{Math Problem Set \#3}} \\
  Charlie Walker
\end{flushleft}

\vspace{5mm}

\noindent\textbf{4.2}\\
\begin{equation*}
D= \begin{bmatrix}
	0 & 1 & 0\\
	0 & 0 & 2\\
	0 & 0 & 0
\end{bmatrix}
\end{equation*}
$D$ is upper triangular, so the eigenvalues are its diagonal entries $\implies D$ has no eigenvalues. Algebraic and geometric multiplicities are both zero.\\

\noindent\textbf{4.4}\\
(i) For non-zero $x \in V, \lambda \in \mathbb{F}$,
\begin{align*}
Ax &= \lambda x\\
(Ax)^H &= (\lambda x)^H\\
x^HA^H &= x^H\lambda ^H
\end{align*}
Post-multiplying by $x$,
\begin{align*}
x^HA^Hx &= x^H\lambda^Hx
\end{align*}
Since A is Hermitian, $A^H = A$,
\begin{align*}
\implies x^HAx &= x^H\lambda^Hx\\
x^H\lambda x &= \lambda^H x^Hx\\
\lambda x^H x &= \lambda^H x^H x\\
\implies \lambda &= \lambda^H
\end{align*}
Thus, $\lambda \in \mathbb{R}$.\\

(ii) $\langle Ax, x \rangle = \langle \lambda x, x \rangle = \bar{\lambda} \|x\|^2 = \langle x, A^H x \rangle = - \langle x, Ax \rangle = -\lambda \|x\|^2 \implies \bar{\lambda} = -\lambda \implies \lambda$ is imaginary. \\

\noindent\textbf{4.6}\\
Take an upper triangular matrix $A \in M_n(\mathbb{F})$,
\begin{equation*}
A= \begin{bmatrix}
	\lambda_1 & & & \\
	& \lambda_2 & & \\
	& & \ddots & \\
	& & & \lambda_n\\
\end{bmatrix}
\end{equation*}
With arbitrary entries above the diagonal, and zeros below. Take any $\lambda \in \mathbb{F}$. Then,
\begin{equation*}
\lambda I - A = \begin{bmatrix}
	\lambda - \lambda_1 & & & \\
	& \lambda - \lambda_2 & & \\
	& & \ddots & \\
	& & & \lambda - \lambda_n\\
\end{bmatrix}
\end{equation*}
$\lambda$ is an eigenvalue of $A$ if and only if $\lambda I - A$ is not invertible. Equivalently, for upper triangular matrices, $A$ is invertible if and only if all the diagonal entries are nonzero. Thus, $\lambda$ is an eigenvalue if and only if it equals one of the numbers $\lambda_1, \lambda_2 \cdots \lambda_n$. Thus, $\lambda_1, \lambda_2 \cdots \lambda_n$ are the eigenvalues of $A$.\\

\noindent\textbf{4.8}\\
(i) $S = \{\sin(x), \cos(x), \sin(2x), \cos(2x)\}$ forms a basis if it's linearly independent, and if every vector $v \in V$ can be expressed as a linear combination of the elements of $S$. \\

\noindent(ii) \begin{equation*}
D = \begin{bmatrix}
	0 & 1 & 0 & 0\\
	1 & 0 & 0 & 0\\
	0 & 0 & 0 & 2\\
	0 & 0 & 2 & 0
\end{bmatrix}\\
\end{equation*}

\noindent(iii) The kernel of $D$ is one of them. \\

\noindent\textbf{4.13}\\
\begin{align*}
A&=\begin{bmatrix}
	0.8 & 0.4\\
	0.2 & 0.6 \\
\end{bmatrix}\\
p(\lambda) &= \det(\lambda I - A)\\
&= \begin{vmatrix}
	\lambda - 0.8 & -0.4\\
	-0.2 & \lambda - 0.6 \\
\end{vmatrix}\\
\implies \sigma(A) &= \{1, 0.4\}
\end{align*}
The eigenvectors corresponding to eigenvalues 1 and 0.4 are $\begin{bmatrix}2 &1\end{bmatrix}^T$ and $\begin{bmatrix}-1& 1\end{bmatrix}^T$, respectively. Thus, the transition matrix is:
\begin{equation*}
P=\begin{bmatrix}
	2 & -1\\
	1 & 1\\
\end{bmatrix}
\end{equation*}\\

\noindent\textbf{4.15}\\
Let set $S = \{x_1, x_2, \cdots , x_n\}$ form an eigenbasis of $A \in M_n(\mathbb{F})$. Because $A$ is semisimple, it is diagonalizable. Let $D = P^{-1}AP$ be the diagonal representation of $A$. Thus,
\begin{align*}
f(A) &= a_0I + a_1A + \cdots + a_nA^n \\
&= a_0I + a_1P^{-1}DP + \cdots + a_n P^{-1}D^nP
\end{align*}
The eigenvalues of $D$ are the entries along its diagonal. For $i = 1, \cdots , n$,
\begin{align*}
\lambda_i &= a_0 + a_1 \lambda_i + \cdots + a_n \lambda_i^n\\
&= f(\lambda_i)
\end{align*}
as desired.\\

\noindent\textbf{4.16}\\
(i) \begin{align*}
A&=\begin{bmatrix}
	0.8 & 0.4\\
	0.2 & 0.6 \\
\end{bmatrix}\\
&= \begin{bmatrix}
	2 & -1 \\
	1 & 1 \\
\end{bmatrix}^{-1}
B \begin{bmatrix}
	2 & -1\\
	1 & 1 \\
\end{bmatrix}\\
&= \begin{bmatrix}
	2 & -1 \\
	1 & 1 \\
\end{bmatrix}^{-1}
 \begin{bmatrix}
	1 & 0\\
	0 & 0.4\\
\end{bmatrix} \ \begin{bmatrix}
	2 & -1\\
	1 & 1 \\
\end{bmatrix}\\
\implies A^n &= \begin{bmatrix}
	1.5 & -1 \\
	-0.5 & 2 \\
\end{bmatrix}
\begin{bmatrix}
	1 & 0\\
	0 & 0.4^n\\
\end{bmatrix} \begin{bmatrix}
	2 & -1\\
	1 & 1 \\
\end{bmatrix}\\
\end{align*}
As $n \rightarrow \infty, 0.4 \rightarrow 0$.
\begin{align*}
\implies \lim_{n \rightarrow \infty} A^n &=
\begin{bmatrix}
	1.5 & -1 \\
	-0.5 & 2 \\
\end{bmatrix}
\begin{bmatrix}
	1 & 0\\
	0 & 0\\
\end{bmatrix} \begin{bmatrix}
	2 & -1\\
	1 & 1 \\
\end{bmatrix}\\
&= \begin{bmatrix}
	1.5 & 0 \\
	-0.5 & 0 \\
\end{bmatrix}\\
&= \begin{bmatrix}
	3 & -1.5 \\
	-1 & 0.5 \\
\end{bmatrix}\\
\implies \| \lim_{n \rightarrow \infty} A^n \|_1 &= 4
\end{align*}\\

\noindent(ii) \begin{align*}
\| \lim_{n \rightarrow \infty} A^n \|_{\infty} &= 4.5\\
\| \lim_{n \rightarrow \infty} A^n \|_{F} &= \sqrt{tr(A^TA)}\\
&= \sqrt{12.5}\\
\end{align*}\\

\noindent (iii) Let matrix $B = 3I + 5A + A^3$, with $A$ as defined above.
\begin{align*}
\sigma(A) &= \{1, 0.4\}\\
\sigma(B) &= \{3 + 5(1) + (1)^3, 3 + 5(0.4) + (0.4)^3\}\\
&= \{9, 5.064\}
\end{align*}\\

\noindent\textbf{4.18}
\begin{equation*}
Ax = \lambda x
\end{equation*}
Left-multiplying,
\begin{align*}
x^TAx &= x^T \lambda x \\
x^TAx &= \lambda x^T x\\
\implies x^TA &= \lambda x^T
\end{align*}

\noindent\textbf{4.20}\\
\begin{align*}
B &= U^HAU\\
B^H &= (U^HAU)^H\\
&= U^HAU^{HH}\\
&= U^HAU\\
&= B\\
\end{align*}
$\implies B$ is Hermitian.\\

\noindent\textbf{4.24}\\
(i) \begin{align*}
p(x) &= \frac{\langle x, Ax \rangle}{\|x\|^2} = \frac{\langle Ax, x \rangle}{\|x\|^2}\\
&\implies \frac{\lambda \langle x, x \rangle}{\|x\|^2} = \frac{\bar{\lambda} \langle x, x \rangle}{\|x\|^2}\\
&\implies \lambda = \bar{\lambda}\\
&\implies \lambda \in \mathbb{R}
\end{align*}\\

\noindent (ii) \begin{align*}
p(x) &= \frac{\langle x, Ax \rangle}{\|x\|^2} = \frac{\langle -Ax, x \rangle}{\|x\|^2}\\
&\implies \frac{\lambda \langle x, x \rangle}{\|x\|^2} = \frac{-\bar{\lambda} \langle x, x \rangle}{\|x\|^2}\\
&\implies -\lambda = \bar{\lambda}\\
&\implies \lambda \in \mathbb{C}
\end{align*}\\

\noindent\textbf{4.25}\\
Any $x_j, j=1,\cdots,n$ can be written as $x_1x_1^Hx_j + \cdots + x_nx_n^Hx_j = (x_1x_1^H + \cdots + x_nx_n^H)x_j \implies x_1x_1^H + \cdots + x_nx_n^H = I_n$.\\

\noindent\textbf{4.27}\\
Positive definite implies that $x^TAx > 0$ for all $x \in \mathbb{R}^n$. Let $e_1, \cdots , e_n$ be the standard basis. For positive definite matrix $A, e_i^TAe_i > 0, i = 1, \cdots, n$. For some basis vector $e_i, e_i^TAe_i = a_{ii}$, that is, the $i^{th}$ entry along the diagonal of A. Thus, the diagonal entries of $A$ are all strictly greater than zero.\\

\noindent\textbf{4.28}\\

\noindent\textbf{4.31}\\
(i) \begin{align*}
\|A\|_2 = \sup_{x \ne 0} \frac{\|Ax\|_2}{\|x\|_2} &= \sup_{x \ne 0} \frac{\|U \Sigma V' x\|_2}{\|x\|_2}\\
&= \sup_{x \ne 0} \frac{\| \Sigma V' x\|_2}{\|x\|_2}\\
&= \sup_{y \ne 0} \frac{\|\Sigma y\|_2}{\|Vy\|_2}\\
&= \sup_{y \ne 0} \frac{(\sum_{i=1}^{r} \sigma_i^2 |y_i|^2)^{\frac{1}{2}}}{(\sum_{i=1}^{r} |y_i|^2)^\frac{1}{2}}\\
& \leq \sigma_1
\end{align*}
For $y=\begin{bmatrix}  1 & 0 & \cdots & 0 \end{bmatrix}^T$, $\|\Sigma y\|_2 = \sigma_1$, and the supremum is attained.\\

\noindent (ii) \begin{align*}
\|A\|_2 = \sup_{x \ne 0} \frac{\|A^{-1}x\|_2}{\|x\|_2} &= \sup_{x \ne 0} \frac{\|(U \Sigma V')^{-1} x\|_2}{\|x\|_2}\\
&= \sup_{x \ne 0} \frac{\|V \Sigma U^T x\|_2}{\|x\|_2}\\
&= \sup_{y \ne 0} \frac{\|\Sigma^{-1} y\|_2}{\|Vy\|_2}\\
&= \sup_{y \ne 0} \frac{(\sum_{i=1}^{r} \frac{1}{\sigma_i^2} |y_i|^2)^{\frac{1}{2}}}{(\sum_{i=1}^{r} |y_i|^2)^\frac{1}{2}}\\
&\implies \|A^{-1}\|_2 = \sigma_n^{-1}
\end{align*}\\

\noindent (iii) \\

\noindent (iv) \begin{align*}
\|UAV\|_2 &= \sup \frac{\|UAVx\|_2}{\|x\|_2}\\
&= \sup \frac{\langle UAVx, x \rangle ^\frac{1}{2}}{\|x\|_2}\\
&= \sup \frac{\langle Ax, x \rangle ^\frac{1}{2}}{\|x\|_2}\\
&= \sup \frac{\|Ax\|_2}{\|x\|_2}\\
&= \|A\|_2
\end{align*}\\

\noindent\textbf{4.32}\\
\begin{align*}
\|UAV\|_F &= \sqrt{Tr[(UAV)(UAV)^H]}\\
&= \sqrt{Tr(UAVV^HA^HU^H)}\\
&= \sqrt{Tr(UAA^HU^H)}\\
&= \sqrt{Tr(AA^HUU^H)}\\
&= \sqrt{Tr(AA^H)}\\
&= \sqrt{Tr(A^2)}\\
&= \|A\|_F
\end{align*}\\

\noindent\textbf{4.33}\\

\noindent\textbf{4.36}\\
The matrix
\[A = \begin{bmatrix} 
0 & 1\\
-1 & 0\\
\end{bmatrix}\]
Has $\det(A) = -1$, eigenvalues $\{i, -i \}$, and singular values $\{1, 1\}$.\\

\noindent\textbf{4.38}\\
(i) \begin{align*}
AA^\dagger &= U_1\Sigma_1V_1^HV_1\Sigma_1^{-1}U_1^H\\
&= I\\
&\implies AA^\dagger A = A
\end{align*}\\

\noindent(ii) $AA^\dagger = I \implies A^\dagger A A^\dagger = A^\dagger$.

\noindent(iii) \begin{align*}
(AA^\dagger)^H &= A^{\dagger H}A^H\\
&= (V_1\Sigma_1^{-1}U_1^H)^H(U_1\Sigma_1V_1^H)^H\\
&= U_1E_1^{-1}V_1^HV_1\Sigma_1U_1^H\\
&=AA^\dagger
\end{align*}\\

\noindent (iv)\begin{align*}
(A^\dagger A)^H &= A^HA^{\dagger H}\\
&= (U_1\Sigma_1V_1^H)^H(V_1 \Sigma_1^-1U_1^H)^H\\
&= V_1\Sigma_1U_1^HU_1\Sigma_1^{-1}V^H\\
&= A^\dagger A
\end{align*}






\end{document}
